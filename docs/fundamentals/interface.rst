Deepforge Interface
===================
The Deepforge editor interface is separated into six views for defining all of the necessary features of your desired project. The details of each interface tab are detailed below. You can switch to any of the views at any time by clicking the appropriate icon on the left side of the screen. In order, the tabs are:

+---------------+--------------------------+
| |tabs|        | - Pipelines_             |
|               | - Executions_            |
|               | - Resources_             |
|               | - Artifacts_             |
|               | - `Custom Utils`_        |
|               | - `Custom Serialization`_|
+---------------+--------------------------+

.. |tabs| image:: interface_tabs.png

Pipelines
---------
.. figure:: pipelines_tab.png
    :align: center
    :width: 75%

In the initial view, all pipelines that currently exist in the project are displayed. New pipelines can be created using the red plus symbol in the bottom right. From this screen, existing pipelines can also be opened for editing, deleted, or renamed. Pipelines in this list are arranged automatically by the system and cannot be manually reordered in the current implementation.

Pipeline editing
~~~~~~~~~~~~~~~~
.. figure:: pipeline_example.png
    :align: center
    :width: 50%

Pipelines are composed of a directed graph of nodes, where each node is an isolated python module. Nodes are added to a pipeline using the red plus button in the bottom right of the workspace. Any nodes that have previously been defined in the project can be added to the pipeline, or new operations can be created when needed. Arrows in the workspace indicate the passing of data between nodes. These arrows can be created by clicking on the desired output (bottom circles) of the first node before clicking on the desired input (top circles) of the second node. Clicking on a node also gives the options to delete (red X), edit (blue </>), or change attributes. Information on the editing of nodes can be found in `Custom Operations <custom_operations.rst>`_

Pipelines are executed by clicking the yellow play button in the bottom right of the workspace. In the window that appears, you can name the execution, select a computation platform, and select a storage platform. The computation platform can either be SciServer's Compute service or a WebGME platform. The available storage platforms are SciServer's Files service and Amazon's S3 service. The provided storage option will be used for storing both the output objects defined in the pipeline, as well as all files used in execution of the pipeline. Login credentials will be required for SciServer computation service, either storage service, and each individual input node in the pipeline.

.. figure:: execute_pipeline.png
    :align: center
    :width: 75%

Executions
----------
.. figure:: executions_tab.png
    :align: center
    :width: 75%

This view allows the review of previous pipeline executions. Clicking on any execution will display any plotted data generated by the pipeline, and selecting multiple executions will display all of the selected plots together. Clicking the provided links will open either the associated pipeline or a trace of the execution (shown below). The blue icon in the top right of every node allows viewing the text output of that node. The execution trace can be viewed during execution to check the status of a running job. During execution, the color of a node indicates its current status. The possible statuses are:

- **Dark gray**: Awaiting initialization
- **Light gray**: Awaiting execution
- **Yellow**: Currently executing
- **Green**: Successfully finished execution
- **Red**: Execution failed

.. figure:: execution_finished.png
    :align: center
    :width: 50%

Resources
---------
.. figure:: resources_tab.png
    :align: center
    :width: 75%

This view shows the available neural network resources available to your pipelines. From this view, resources can be created, deleted, and renamed. Resources are arranged by the deepforge system and cannot by manually reordered.

.. figure:: neural_network.png
    :align: center
    :width: 50%

As with pipelines, the neural networks are depicted as directed graphs. Each node in the graph corresponds to a single layer or operation in the network (information on operations can be found on the `keras website <https://keras.io/api/>`_). Clicking on a node provides the ability to change the attributes of that layer, delete the layer, or add new layers before or after the current node. Many operations require that certain attributes be defined before use. The Conv2D node pictured above, for example, requires that the *filters* and *kernel_size* attributes be defined. If these are left as *<none>*, a visual indicator will show that there is an error to help prevent mistakes. In order to ease analysis and development, hovering over any connecting line will display the shape of the data as it moves between the given layers.

Artifacts
---------
.. figure:: artifacts_tab.png
    :align: center
    :width: 75%

In this view, you can see all artifacts that are available to your pipelines. These artifacts can be used in any pipeline through the inclusion of the built in **Input** node. Artifacts are, by default, only supported in the form of either keras models (such as those created using the `keras.model.save_model <https://keras.io/api/models/model_saving_apis/#save_model-function>`_ function) or python pickle objects. Other artifact types can also be used, but require the definition of a `custom serialization <Custom Serialization_>`_. A new artifact can be created in one of three ways. First, artifacts are automatically during the execution of any pipeline that includes the built-in **Output** node. Second, artifacts can be directly uploaded in this view using the red upload button in the bottom right of the workspace. Using this option will also upload the artifact to the storage platform specified in the popup window. Finally, artifacts that already exist in one of the storage platforms can be imported using the blue import button in the bottom right of the workspace.

|import| |upload|

.. |import| image:: import_artifact.png
    :width: 45%
.. |upload| image:: upload_artifact.png
    :width: 45%


Custom Utils
------------
.. figure:: custom_utils.png
    :align: center
    :width: 75%

This view allows the creation and editing of custom utility modules. Utilities created here can be imported into any pipeline node. For example, the *swarp_config_string* shown above can be printed out in a node using the following code:

.. code-block:: python

    import utils.swarp_string as ss
    print(ss.swarp_config_string)

Custom Serialization
--------------------
.. figure:: custom_serializer.png
    :align: center
    :width: 75%

In this view, you can create custom serialization protocols for the creation and use of artifacts that are neither python pickle objects nor keras models. To create a serialization, you will need to define two functions, one for serialization and one for deserialization. These functions must then be passed as arguments to the *deepforge.serialization.register* function as shown in the commented code above. The serializer and deserializer should have the same signatures as the dump and load functions respectively from python's `pickle module <https://docs.python.org/3/library/pickle.html>`_.
